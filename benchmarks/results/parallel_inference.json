{
  "meta": {
    "version": "0.2.5",
    "timestamp": "2026-02-04T05:00:00.000000Z",
    "cpu": "AMD Ryzen 9 7845HX with Radeon Graphics",
    "cores": 24,
    "ram_gb": 63.2,
    "os": "Windows 10.0.26100",
    "test": "parallel_inference",
    "total_threads_used": 24,
    "concurrent_requests": 3,
    "threads_per_request": 8,
    "model": "BitNet-b1.58-2B-4T",
    "model_size_gib": 1.10,
    "model_params_b": 2.41,
    "quantization": "I2_S (2 bpw ternary)",
    "bitnet_build": "3962 (1f86f058) with Clang 20.1.8",
    "generated_tokens_per_job": 128,
    "temperature": 0
  },
  "results": {
    "job1": {
      "prompt": "The future of artificial intelligence is",
      "prompt_tokens": 7,
      "prompt_tokens_per_sec": 14.02,
      "tokens_per_sec": 13.54,
      "ms_per_token": 73.84,
      "model_load_time_ms": 848.91,
      "total_time_ms": 9926.84
    },
    "job2": {
      "prompt": "Climate change will impact humanity by",
      "prompt_tokens": 7,
      "prompt_tokens_per_sec": 13.91,
      "tokens_per_sec": 13.60,
      "ms_per_token": 73.54,
      "model_load_time_ms": 797.27,
      "total_time_ms": 9892.35
    },
    "job3": {
      "prompt": "The best programming language for beginners is",
      "prompt_tokens": 8,
      "prompt_tokens_per_sec": 13.43,
      "tokens_per_sec": 13.72,
      "ms_per_token": 72.88,
      "model_load_time_ms": 726.07,
      "total_time_ms": 9899.45
    },
    "combined_throughput_tokens_per_sec": 40.86,
    "wall_clock_time_ms": 11017.01,
    "single_stream_8t_tokens_per_sec": 36.94,
    "sequential_estimated_time_ms": 10439.52,
    "speedup_vs_sequential": 0.95
  },
  "analysis": {
    "combined_throughput_vs_single": "40.86 t/s combined (3 streams) vs 36.94 t/s single-stream = 1.11x aggregate throughput",
    "per_stream_degradation": "Each stream drops from 36.94 t/s to ~13.62 t/s (63% degradation per stream)",
    "wall_clock_efficiency": "Wall clock 11.0s vs estimated sequential 10.4s (3 x 3.47s per single run) = 0.95x, slight overhead from resource contention",
    "bottleneck": "With 3x8=24 threads fully saturating all cores, each stream competes for L2/L3 cache and memory bandwidth. The 1-bit LUT kernels are memory-bound, so parallel execution provides minimal aggregate throughput gain on this architecture.",
    "recommendation": "For ARIA distributed inference: serialize requests per node (1 stream at a time with 8 threads) for best per-request latency. Use multi-node parallelism instead of intra-node parallelism to scale throughput."
  }
}
